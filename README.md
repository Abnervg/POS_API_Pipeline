# **ğŸ§¾ Automated POS Sales Data Pipeline & Reporting**

## **ğŸ” Project Overview**

This project implements a complete, production-grade data engineering pipeline that automates the extraction, transformation, and analysis of transactional data from a Point of Sale (POS) system.

Raw JSON data is fetched incrementally from the POS API, processed using a robust Python and Pandas-based ETL, and loaded into a partitioned data lake on AWS S3. The entire workflow is containerized with **Docker** and orchestrated with **Apache Airflow**, with automated daily runs scheduled via **GitHub Actions**. The pipeline culminates in the generation of automated monthly and cumulative business intelligence reports, providing actionable insights into sales trends, product performance, and customer behavior.

## **âœ¨ Key Features**

* **Incremental Data Extraction**: Intelligently fetches only new receipts since the last successful run, using a persistent state file to track progress.  
* **Automated Orchestration**: The entire pipeline is managed by an Apache Airflow DAG, which handles scheduling, dependency management, and error retries.  
* **Containerized Environment**: The application is fully containerized with Docker, ensuring consistency and portability between development and production environments.  
* **Advanced Data Transformation**: Includes sophisticated cleaning logic to handle complex, nested JSON data, such as "exploding" combo meal items into individual rows for accurate sales counting.  
* **Partitioned Data Lake**: Curated data is saved in the efficient Parquet format and partitioned by year and month in an AWS S3 data lake for optimized querying.  
* **Automated Reporting**: Generates monthly and cumulative reports in Markdown format, complete with embedded visualizations that highlight key business metrics and trends.  
* **CI/CD Automation**: A GitHub Actions workflow automates the daily execution of the ETL pipeline, providing a serverless and reliable scheduling solution.

## **ğŸ› ï¸ Tech Stack**

| Layer | Tool/Service |
| :---- | :---- |
| **Orchestration** | Apache Airflow |
| **Containerization** | Docker, Docker Compose |
| **CI/CD & Scheduling** | GitHub Actions |
| **Language** | Python |
| **Data Processing** | Pandas |
| **Cloud Storage** | AWS S3 (Data Lake) |
| **Python Libraries** | Requests, Boto3, s3fs, PyArrow, Seaborn, Matplotlib, python-dotenv |
| **Testing** | Pytest |

## **ğŸ—‚ï¸ Project Structure**

POS\_API\_Pipeline/  
â”œâ”€â”€ .github/  
â”‚   â””â”€â”€ workflows/  
â”‚       â””â”€â”€ monthly\_etl.yml   \# GitHub Actions workflow for scheduling  
â”œâ”€â”€ config/  
â”‚   â”œâ”€â”€ config.env          \# For local configuration (ignored by Git)  
â”‚   â””â”€â”€ etl\_state.json        \# Tracks last extraction timestamp (ignored by Git)  
â”œâ”€â”€ dags/  
â”‚   â””â”€â”€ incremental\_etl\_dag.py \# Airflow DAG definition  
â”œâ”€â”€ data/  
â”‚   â”œâ”€â”€ raw/                \# Stores raw extracted JSON data locally  
â”‚   â””â”€â”€ curated/            \# Stores final processed data locally  
â”œâ”€â”€ etl/  
â”‚   â”œâ”€â”€ \_\_init\_\_.py  
â”‚   â”œâ”€â”€ extract.py  
â”‚   â”œâ”€â”€ transform.py  
â”‚   â””â”€â”€ load.py  
â”œâ”€â”€ reporting/  
â”‚   â”œâ”€â”€ \_\_init\_\_.py  
â”‚   â”œâ”€â”€ data\_preparation.py  
â”‚   â”œâ”€â”€ monthly\_report.py  
â”‚   â””â”€â”€ cumulative\_report.py  
â”œâ”€â”€ tests/  
â”‚   â””â”€â”€ ...                 \# Unit tests for the pipeline  
â”œâ”€â”€ .gitignore  
â”œâ”€â”€ Dockerfile              \# Defines the application's container image  
â”œâ”€â”€ docker-compose.yml      \# For running the local Airflow environment  
â”œâ”€â”€ main.py                 \# Main script orchestrator (callable from CLI)  
â””â”€â”€ requirements.txt        \# Python dependencies  
